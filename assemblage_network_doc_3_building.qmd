---
title: "From similarity to network: Building graphs for community detection"
subtitle: "Data Science in Archaeology, Part III.a."
author: "James Scott Cardinal"
date: today
format: html
editor: visual
bibliography: references.bib
---

![Why is there math in my archaeology? ...or is the archaeology in my math? (Image by Author)](archaeo_DS_mine.png){fig-alt="Archaeology and Data Science" fig-align="center"}

It has been a little while since my last installment ([Part II](https://towardsdatascience.com/similarity-measures-and-graph-adjacency-with-sets-a33d16e527e1 "Similarity Measures and Graph Adjacency with Sets")). Projects, deadlines, and (of course) life has kept me overly busy. My thanks for your patience, and I'll try and make it worth the wait.

Last time, we talked about different similarity metrics for sets or combinatorial problems, and the particular strengths and idiosyncrasies of each. We were trying to find the best way to project our bipartite graph of archaeological provenience and artifact types into individual one-mode graphs. Our goal is to find communities in each of those projected graphs to find meaningful *assemblages* (communities of artifacts) and *contexts* (communities of provenience locations).

That does, however, require that we talk about how we determine "best" for this particular scenario. Since our goal is community detection, we want to choose the metric that connects vertices that have the strongest associations while *avoiding* spurious or non-informative edges. I mentioned last time that we are, in part, looking to see if our projected graphs exhibit characteristics of a *scale-free* network. We'll talk about that more later in this post, but it boils down to finding a network where most nodes have a few strong connections rather than a lot of weak ones.

Since this is likely to be a fairly large and involved topic, I'm breaking Part III into two posts. This one (Part III.a.) will talk about translating our similarity measures into a network, and building the graphs. The next one (Part III.b.) will tackle the community detection methods, of which there are many.

For those just joining in, this is part of the third installment of a series on archaeological data science covering:

-   [Part I](https://medium.com/p/648a2f20d389 "Bipartite Graphs for Archaeological Assemblage Networks") -- Creating and exploring bipartite and one-mode graphs,

-   [Part II](https://towardsdatascience.com/similarity-measures-and-graph-adjacency-with-sets-a33d16e527e1 "Similarity Measures and Graph Adjacency with Sets") -- Similarity measures for sets and graph adjacency,

-   Part III -- Graph structure and community detection methods,

    -   a.) Scale-free and small world networks

    -   b.) Community detection methods

-   Part IV -- Geo-spatial networks

I'll continue using R for the [coding](https://github.com/archaeojsc/assemblage_network "Assemblage Network GitHub"), but all of this could be done with Python just as easily.

## Introduction {#sec-introduction}

In [Part II](https://towardsdatascience.com/similarity-measures-and-graph-adjacency-with-sets-a33d16e527e1 "Similarity Measures and Graph Adjacency with Sets"), we went into some depth about what it means for sets to be *similar* and some ways to calculate a metric for that similarity. This time around, we're going to look more closely into how to decide what it means to be similar *enough*. In other words, we want to find an analytically valid way to decide on a *threshold* of similarity that best captures the real connections between entities while avoiding spurious associations. That will also help to guide us in selecting which of the similarity metrics to use (i.e., overlap, Jaccard, or Sørensen--Dice) as we go on to start building our bipartite graph projections.

### A quick recap... {#sec-a-quick-recap...}

We started (in [Part I](https://medium.com/p/648a2f20d389 "Bipartite Graphs for Archaeological Assemblage Networks")) by building a bipartite graph between artifact types and their locations, shown in @fig-bipartite-graph. To keep things simple, we're using an *un-*weighted bipartite graph to look at co-locations of artifact types.

```{r}
#| label: code-bpg
#| echo: false
#| message: false

require(tidyverse)
require(igraph)
require(ggraph)

# Import data from file ---------------------------------------------------

dat_raw <- read_csv("./data/Catalog_SiteA.csv",
                col_select = c(LEVEL_ID, CODE))

# Artifact to exclude from analysis
exclude_artifacts <-
  as.vector(read.csv("./data/code_exclude.csv", header = T)$x)

# Remove Bakelite and plastic buttons from exclusion list
exclude_artifacts <-
  exclude_artifacts[!exclude_artifacts %in% c("BKLT", "PB")]

# Import list of artifact codes
artifact_codes <-
  read.csv("./data/code_list.csv",
           header = TRUE,
           stringsAsFactors = FALSE)

# Filter data set for excluded artifact types
dat <- dat_raw %>% filter(!(CODE %in% exclude_artifacts))

# Create un-weighted bipartite graph --------------------------------------

g_assemblages_bpg <-
  graph_from_data_frame(unique.data.frame(select(dat, LEVEL_ID, CODE)),
                        directed = FALSE)

V(g_assemblages_bpg)$type <-
  bipartite_mapping(g_assemblages_bpg)$type

```

```{r}
#| label: fig-bipartite-graph
#| fig-cap: "Bipartite network of Provenience and Artifact Type."
#| fig-alt: "Bipartite graph plot of proveniences and artifact types."
#| echo: false
#| message: false
#| warning: false

g_assemblages_bpg_layout <-
  g_assemblages_bpg %>% layout_as_bipartite()

g_assemblages_bpg %>%
  ggraph(layout = g_assemblages_bpg_layout) +
  geom_edge_link(edge_color = "darkgray", edge_alpha = 0.25) +
  geom_node_point(aes(color = type)) +
  scale_color_manual(
    values = c("darkgreen", "darkblue"),
    name = "Node Type",
    breaks = c(FALSE, TRUE),
    labels = c("Provenience", "Artifact")
  ) +
  ggtitle("Bipartite network of Provenience and Artifact Type")


```

Looking at the bipartite graph, it seems apparent that there are some natural clusters or groupings. We found, though, that projecting this graph into its one-mode counterparts resulted in very densely connected networks.

Next, in [Part II](https://towardsdatascience.com/similarity-measures-and-graph-adjacency-with-sets-a33d16e527e1 "Similarity Measures and Graph Adjacency with Sets"), we looked at different similarity metrics to find a better method of projection that the simple overlap provided in `igraph`. We looked at the similarities calculated with the Szymkiewicz-Simpson overlap coefficient, Jaccard similarity coefficient, and Sørensen--Dice coefficient for both provenience (@fig-sim-prov) and artifact types (@fig-sim-artifact).

```{r}
#| label: code-reestablish-environment
#| echo: false
#| message: false
#| warning: false

# Create incidence matrix from bipartite graph -----------------------------

g_assemblages_bpg_inc <- as_incidence_matrix(g_assemblages_bpg)

# Project one-mode graphs, Szymkiewicz-Simpson -----------------------------

overlap_coef_bin <- function(x) {
  # Calculate the pairwise sums of non-zero matrix elements to find the number
  # of intersecting elements between each input column
  bin_intersect_mat <- t(x) %*% x
  
  # Calculate the input column sums to find the individual size of each set
  col_sum <- apply(x, 2, function(xx)
    sum(xx != 0))
  
  # Find the smaller of each pair of sets by taking the matrix outer minimum
  min_set_size_mat <- outer(col_sum, col_sum, FUN = pmin)
  
  # Szymkiewicz-Simpson overlap coefficient is the pairwise intersection of two
  # sets divided by the size of the smaller set
  res <- bin_intersect_mat / min_set_size_mat
  
  # Set diagonal to identity
  diag(res) <- 1L
  
  # Assign input column names to rows and columns of return matrix
  dimnames(res) <- list(colnames(x), colnames(x))
  
  return(res)
}


## Project provenience -----------------------------------------------------

sim_prov_ssoc <- overlap_coef_bin(t(g_assemblages_bpg_inc))

prov_ssoc_vals <-
  sim_prov_ssoc[lower.tri(sim_prov_ssoc, diag = FALSE)]

## Project artifact types --------------------------------------------------

sim_artifact_ssoc <- overlap_coef_bin(g_assemblages_bpg_inc)

artifact_ssoc_vals <-
  sim_artifact_ssoc[lower.tri(sim_artifact_ssoc, diag = FALSE)]

# Project one-mode graphs, Sorenson-Dice -----------------------------------

soren_dice_sim_bin <- function(x) {
  # Calculate the pairwise sums of non-zero matrix elements to find the number
  # of intersecting elements between each input column
  bin_intersect_mat <- t(x) %*% x
  
  # Calculate the input column sums, to find the individual size of each set
  col_sum <- apply(x, 2, function(xx)
    sum(xx != 0))
  
  # Calculate the matrix outer sums for pairwise sum of set sizes
  set_size_sum_mat <- outer(col_sum, col_sum, FUN = "+")
  
  # Sorenson-Dice index is twice the size of the intersection divided by the
  # sum of the size for each set
  res <- (2 * bin_intersect_mat) / set_size_sum_mat
  
  # Set diagonal to identity
  diag(res) <- 1L
  
  # Assign input column names to rows and columns of return matrix
  dimnames(res) <- list(colnames(x), colnames(x))
  return(res)
  
}

## Project provenience -----------------------------------------------------

sim_prov_sd <- soren_dice_sim_bin(t(g_assemblages_bpg_inc))

prov_sd_vals <-
  sim_prov_sd[lower.tri(sim_prov_sd, diag = FALSE)]

## Project artifact types --------------------------------------------------

sim_artifact_sd <- soren_dice_sim_bin(g_assemblages_bpg_inc)

artifact_sd_vals <-
  sim_artifact_sd[lower.tri(sim_artifact_sd, diag = FALSE)]

# Project one-mode graphs, Jaccard ----------------------------------------

jaccard_sim_bin <- function(x) {
  # Calculate the pairwise sums of non-zero matrix elements to find the number
  # of intersecting elements between each input column
  bin_intersect_mat <- t(x) %*% x
  
  # Calculate the input column sums to find the individual size of each set
  x_col_sum <- apply(x, 2, function(xx)
    sum(xx != 0))
  
  # Calculate the matrix outer sums for pairwise sum of set sizes
  set_size_sum_mat <- outer(x_col_sum, x_col_sum, FUN = "+")
  
  # Jaccard index is intersection of set sizes over the size of the union of
  # sets
  res <- bin_intersect_mat / (set_size_sum_mat - bin_intersect_mat)
  
  # Set diagonal to identity
  diag(res) <- 1L
  
  # Assign input column names to rows and columns of return matrix
  dimnames(res) <- list(colnames(x), colnames(x))
  
  return(res)
}

## Project provenience -----------------------------------------------------

sim_prov_jacc <- jaccard_sim_bin(t(g_assemblages_bpg_inc))

prov_jacc_vals <-
  sim_prov_jacc[lower.tri(sim_prov_jacc, diag = FALSE)]

## Project artifact types --------------------------------------------------

sim_artifact_jacc <- jaccard_sim_bin(g_assemblages_bpg_inc)

artifact_jacc_vals <-
  sim_artifact_jacc[lower.tri(sim_artifact_jacc, diag = FALSE)]

# Comparing similarity measures -------------------------------------------

sim_prov <-
  data.frame(ssoc = prov_ssoc_vals,
             jacc = prov_jacc_vals,
             sd = prov_sd_vals)

sim_artifact <-
  data.frame(ssoc = artifact_ssoc_vals,
             jacc = artifact_jacc_vals,
             sd = artifact_sd_vals)

```

```{r}
#| label: fig-sim-prov
#| fig-cap: "Similarity measures for proveniences."
#| fig-alt: "Similarity measures for proveniences."
#| echo: false
#| message: false
#| warning: false

# Plot densities by similarity measure
sim_prov %>% stack() %>% 
  ggplot(aes(x = values)) +
  geom_density(fill = "darkgreen",
               alpha = 0.4) +
  facet_grid(ind ~ ., scales = "fixed") +
  ggtitle("Similarity Measures for Proveniences")

```

For proveniences, it looks like there are quite a few that have at least some similarities in their artifact content. Looking at the density of scores for each methods, it appears that in fact the *majority* of proveniences have non-zero similarity scores. There are discernible "peaks" at zero and, for Jaccard and Sørensen-Dice at least, most scores are below $0.5$.

```{r}
#| label: fig-sim-artifact
#| fig-cap: "Similarity measures for artifact types."
#| fig-alt: "Similarity measures for artifact types."
#| echo: false
#| message: false
#| warning: false

# Plot densities by similarity measure
sim_artifact %>% stack() %>%
  ggplot(aes(x = values)) +
  geom_density(fill = "darkblue",
               alpha = 0.4) +
  facet_grid(ind ~ ., scales = "free") +
  ggtitle("Similarity Measures for Artifact Types")

```

Unlike the proveniences, the co-location of artifacts is obviously much more sparse and the density plot is dominated by the left-hand "spike" of zero similarities. Looking closely, though, we can notice some apparent "bumps" along the right tails (most obvious for the overlap coefficient) where we seem to have a fair number of higher similarity artifacts. In other words, some artifact types occur together quite often.

```{r}
#| label: fig-sim-artifact-nonzero
#| fig-cap: "Non-zero similarity measures for artifact types."
#| fig-alt: "Non-zero similarity measures for artifact types."
#| echo: false
#| message: false
#| warning: false

# Plot densities by similarity measure
sim_artifact %>% 
  stack() %>%
  filter(values > 0) %>% 
  ggplot(aes(x = values)) +
  geom_density(fill = "darkblue",
               alpha = 0.4) +
  facet_grid(ind ~ ., scales = "free") +
  ggtitle("Non-zero similarity measures for Artifact Types")
```

Filtering the artifact similarities to only display non-zero values, we an see the distribution of similarities a little better within that right tail. Sørensen--Dice and Jaccard scores still concentrate on the lower end of the scale, while it's apparent that the overlap coefficient is rating the similarities considerably higher.

If you remember from the last installment, the overlap coefficient captures subsets and has a more "permissive" approach to scoring similarities. Sørensen--Dice and Jaccard, however, penalize differences in their scoring, which lead to lower overall similarity scores

Now what we want to do is look at how to go about selecting our similarity metric, how to find appropriate thresholds of similarity, and what effects these have on the resulting graph projections. The goal is to find the right metric and threshold that will allow us to detect the underlying structure of the relationships within and between provenience and artifact types.

## Random Graphs versus Real Networks

### Graph Properties

### Graph structure and scale-free networks {#sec-graph-structure-and-scale-free-networks}

What we're trying to do is find out if the inherent structure of our projected graphs, and therefore our bipartite graph as a whole, shows whether or not it is possible to that it could be *random*. If the structure does *not* appear to be random, then we can reasonably infer that some other process is constraining or directing which artifacts appear where.

In archaeological terms, that means that it's likely that there is some intentional organization to the use of space for certain activities or that certain artifact assemblages are associated with different activities. If so, then we can find some partitioning of the artifacts and/or proveniences that have meaningful interpretations.

Mind you, we're not actually trying to *induce* a structure. We are, however, expecting that there has been some manner of degradation in the "signal" of our data (see the introduction to [Part I](https://medium.com/p/648a2f20d389 "Bipartite Graphs for Archaeological Assemblage Networks") as to why). Our selection of similarity and threshold are meant to dampen the "noise" introduced to the site over time.

One way to determine if we have non-random structure in our graph is to find out if it exhibits [*scale-free network*](https://en.wikipedia.org/wiki/Scale-free_network "Scale-free network | WikipediA") properties. A scale-free network is one in which the distribution for the degree of nodes in the graph approximate a power-law distribution, meaning that most nodes in the graphs have few connections while a few "hub" nodes have many. In a random network, there would be a more *even*ly distributed probability of node degrees and hub nodes would be far less likely to occur.

The presence of a scale-free network suggests that there is some latent *non*-random process at play that is driving differential attachments between those nodes. It may be that there is a different process of growth in the network, which archaeologically would be some different spatial or temporal process of artifact deposition. Alternatively, there may be a clustering process or preferential attachment by which certain nodes are more closely associated than others as in the case of hub nodes. A certain suite of artifacts that are all related to the same activities (i.e., an *assemblage*) would cluster together, for example.

### Detecting scale-free networks {#sec-detecting-scale-free-networks}

If a network is scale-free, then the distribution of node degree will approximate a power-law distribution of the form $P_{\text{deg}}(k) \propto k^{-\gamma}$ where $k$ is node degree and $\gamma$ is some exponent. We will know $k$, but we would obviously need to find the appropriate value for $\gamma$ from our empirical distribution. To illustrate, lets look at the probability density function for various values of $\gamma$ (@fig-power-law). We have 251 artifact types in our archaeological sample data, so lets look at the ideal power-law distributions if each node could have a maximum of 251 degrees.

```{r}
#| label: fig-power-law
#| fig-cap: "Power-law distribution for different values of gamma."
#| fig-alt: "Power-law distribution for different values of gamma."
#| echo: false
#| message: false
#| warning: false

powerlaw_pdf <- function(x, gamma = 1) {
  density <- x ^ (-gamma)
  return(density)
}

ggplot(data.frame(x = c(1, 251)), aes(x = x)) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 2), aes(color = "2")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 3), aes(color = "3")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 4), aes(color = "4")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 5), aes(color = "5")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 6), aes(color = "6")) +
  labs(color = "Gamma") +
  ylab("P(k)") + xlab("k") +
  ggtitle("Power-law distributions for different values of gamma")

```

Pretty easy to see that if our artifact type graph were scale free, the vast majority of the nodes would have a *very* small degree. Not so easy to see how much that probability is affected by $\gamma$, though. It's a lot easier to see the effect of $\gamma$ if we take the $\log$ of $P_{\text{deg}}(k)$ (@fig-power-law-log).

```{r}
#| label: fig-power-law-log
#| fig-cap: "Log-scale power-law distribution for different values of gamma."
#| fig-alt: "Log-scale power-law distribution for different values of gamma."
#| echo: false
#| message: false
#| warning: false

ggplot(data.frame(x = c(1, 251)), aes(x = x)) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 2), aes(color = "2")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 3), aes(color = "3")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 4), aes(color = "4")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 5), aes(color = "5")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 6), aes(color = "6")) +
  scale_y_continuous(trans = "log10") +
  labs(color = "Gamma") +
  ylab("log P(k)") + xlab("k") +
  ggtitle("Log-scale power-law distribution for different values of gamma")

```

For every increment of the exponent $\gamma$, the probability of higher-degree nodes decreases much more quickly. This will help in selecting our similarity metric, when we look at which one produces a graph network that approximates this sort of power-law in its degree distribution.

### Why a scale-free network? {#sec-why-a-scale-free-network}

This is, of course, not the only way to look for non-random structure in a graph network. It's reasonable, then, to be wondering why we would expect our archaeological networks should follow this sort of power-law structure. This subject could easily be its own article, to be honest, but the short-form version is that it's because there is a natural connection between information entropy, power-law distributions, and the information communicated about an archaeological site by individual artifact types.[^1] The artifact types found on a site communicate information about the activities that occurred there, and the provenience (i.e., location) of those types tell us about the spatial organization of those activities.

[^1]: See @Justeson1973 for a detailed explanation.

There are, actually, some surprising similarities with how different gene expressions correlate with phenotype. If you think about the co-location of artifact types being something like genetic expressions and a provenience being the individual sample, then the correspondence of artifact "expressions" between samples should indicate that they're from the same "population" of characteristics (i.e., phenotype). The methods being presented in this series of articles originated with my experiments in adapting [weighted gene co-expression network analysis](https://en.wikipedia.org/wiki/Weighted_correlation_network_analysis "Weighted correlation network analysis") to archaeological data.[^2] Scale-free network construction is one of the crucial steps in those methods.

[^2]: For more information about weighted gene co-expression network analysis and scale free networks, see @Zhang2005 and @Langfelder2008b

The biggest difference between the genetic applications, and what we're doing here archaeologically, is that we're aiming to do this along each dimension of the problem. We're first looking to see if there are groups of artifact types (i.e., assemblages) that are separable and reflect either different activities (e.g., domestic versus architectural artifacts) or different temporal occupations (e.g., 18th century versus 19th century). *Then* we're going to go back and see of those assemblages are separated spatially by groups of proveniences (i.e., contexts).

It's a bi-clustering problem in some ways, but we have to first identify rough clusters *within* the locations or artifacts.

## From similarity to adjacency {#sec-from-similarity-to-adjacency}

We could simply use our raw similarity scores to build our networks, but we would still have very dense and noisy graphs. Instead, we want to find a method that boosts the *signal* of our graph structure, while suppressing the *noise* from spurious associations. To do that, we need to filter our similarity scores to form a graph adjacency matrix containing (as much as possible) only the most significant associations between entities. There are essentially two ways we can go about this.

The simplest is to just set a hard cutoff threshold -- e.g., a specific value or quantile -- and build an un-weighted or binary adjacency matrix. If we select the right threshold, this can be very effective and results in a more sparse graph. The downside is that it risks discarding legitimate (albeit lower strength) connections along with the noise, and tends to leave isolated nodes (i.e., nodes that aren't connected to any others).

Another approach is to use a function to calculate a *soft* threshold value, along the lines of an activation function. This results in a weighted graph in which smaller similarity values are pushed closer towards zero, while very high similarity values would stay closer to one. The advantage is that there is less risk of suppressing significant connections. The downside is that the noise is still in there, albeit substantially dampened, and doesn't decrease the graph's overall density. Weighted graphs are also just a bit more complicated to parse for community detection. Some methods consider edge weight as strength of connection, while others require the edge weights to be a measure of distance between nodes.

Thankfully, the two are not mutually exclusive! A hybrid approach would be to boost the similarity signal by employing a soft threshold function, and *then* finding an appropriate hard threshold. This is, in effect, what ultimately happens when you apply a clustering algorithm to a distance matrix. Whether it's selecting the number of clusters or setting a cutoff distance to a dendrogram, you're essentially deciding membership by picking a hard threshold to the measure being evaluated.

Graph community detection is quite closely related to non-graph clustering, particularly hierarchical clustering. Setting a threshold for building the adjacency matrix determines the resulting structure of the graph.

### Hard thresholds {#sec-hard-thresholds}

A hard threshold is the simplest to implement -- any similarity value lower than your threshold goes to zero, and equal or higher goes to one. The trick is in picking that value, which we'll explore a little later, but you can think of it as tuning a model parameter. The objective would be to maximize the strength of connections, minimize node degree, and avoid isolates.

So, lets formally define our hard threshold adjacency matrix[^3] as $A_{i,j}$ , our similarity matrix as $S_{i,j}$, and our threshold value as $\tau$.

[^3]: Adapted from @Zhang2005

$$
A_{i,j} = f(S_{i,j}, \tau) \equiv 
\begin{cases}
0 & \text{if } S_{i,j} < \tau\\
1 & \text{if } S_{i,j} \geq \tau 
\end{cases}
$$ {#eq-signum-adj}

We can easily code that up as a function `hard_threshold_adj`, which takes a similarity matrix `sim_mat` and a threshold value `tau` as arguments and returns a binary adjacency matrix.

```{r}

hard_threshold_adjacency <- function(sim_mat, tau) {
  return(ifelse(sim_mat < tau, 0, 1))
}

```

### Soft thresholds {#sec-soft-thresholds}

Soft thresholds consist of a transformation of the raw similarity score by passing it through a function that boosts stronger similarities and suppresses weaker ones. Those familiar with activation functions in deep learning will already be accustomed to the variety of approaches available -- e.g., sigmoid function, tangent hyperbolic or "tanh" function, etc. Soft thresholds work the same way, transforming the input values into a monotonically increasing output value.

Soft thresholds result in weighted graphs, therefore they do not reduce the graph's density or alter the degree distribution, unless a subsequent (hard) threshold is applied as well. In a weighted network, though, the density is less of an issue, since all metrics become proportional to the associated edge weights. It does, however, become rather more computationally intensive. The advantage of that trade-off is that more of the information about the relationships is retained in the graph's structure.

Since our similarity scores are already in the range $[0,1]$, we don't really need quite the same sort transformation as we would if we needed to take any real number as an input. We just need to "bend" our linear input range to minimize some portion of the smaller values. Luckily, there is a very simple way to do that by using an exponential function. We'll define a power adjacency[^4] function:

[^4]: Again, we're following after @Zhang2005.

$$
Power(S_{i,j}) = |S_{i,j}|^{\beta}
$$ {#eq-power-adj}

This is simply raising the absolute value of our raw input value to some power $\beta$. Our similarity scores are all $[0,1]$ so the absolute value is irrelevant in this case. For other use cases, it would be necessary.[^5]

[^5]: For example, if we were including multiplicities of events (i.e., how *many* of each artifact type was found at each location), then different similarity measures or correlations would be appropriate that might produced signed values.

```{r}

power_adj <- function(x, beta) {
  abs(x) ^ beta
}

```

Increasing $\beta$ can be seen to push the smaller values towards zero, which increases the minimum value at which the adjacency score exceeds $0.5$ (thus, it's a *soft* threshold). For example, at $\beta=2$ only similarity values of $S_{i,j} \geq 0.71$ would break the $0.5$ adjacency threshold and the weights of all lower values penalized. At $\beta=3$ it would take $S_{i,j} \geq 0.79$ and so on.

```{r}
#| label: fig-power-adjacency
#| fig-cap: "Power adjacency distribution for different values of beta."
#| fig-alt: "Power adjacency distribution for different values of beta."
#| echo: false
#| message: false
#| warning: false

ggplot(data.frame(x = seq(0, 1, length.out = 1000)), aes(x = x)) +
  stat_function(fun = power_adj, args = list(beta = 2), aes(color = "2")) +
  stat_function(fun = power_adj, args = list(beta = 3), aes(color = "3")) +
  stat_function(fun = power_adj, args = list(beta = 4), aes(color = "4")) +
  stat_function(fun = power_adj, args = list(beta = 5), aes(color = "5")) +
  stat_function(fun = power_adj, args = list(beta = 6), aes(color = "6")) +
  labs(color = "beta") +
  ylab("Power(x)") + xlab("x")

```

## Choosing threshold type and parameters {#sec-choosing-threshold-type-and-parameters}

So, now that we've explored some similarity and adjacency methods, the question remains -- how do we choose between them? We're looking for non-random structure within our sample data, and we know that the non-random distribution of connections in our network *should* approximate a scale-free network. Therefore, we want to pick the similarity measure and adjacency threshold that result in a graph that best approximates such a network.

How do we do that?

Well, one proven method[^6] is that we can test the fit of a linear regression between the the observed distribution of network connectivity and an ideal power law probability distribution. The higher the $R^2$ of the model fit, the closer the observed data is to a scale-free topology.

[^6]: See @Zhang2005

It's important to note that the meaning of "connectivity" is different for weighted (soft) and un-weighted (hard) adjacency. For un-weighted graphs, connectivity is simply the degree of each node. If the edges are weighted, though, connectivity is the sum of edge weights (or "strength") connected to each node.

Our next step, then, is to evaluate each of our similarity matrices to see which one most closely fits to a scale-free network. Before we do that, though, let's take a look at what the connectivity look like when just using the similarity scores as the network adjacency.

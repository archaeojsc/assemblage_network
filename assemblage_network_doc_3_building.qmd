---
title: "From similarity to network: Building graphs for community detection"
subtitle: "Data Science in Archaeology, Part III.a."
author: "James Scott Cardinal"
date: today
format: html
editor: visual
---

![Why is there math in my archaeology? ...or is the archaeology in my math? (Image by Author)](archaeo_DS_mine.png){fig-alt="Archaeology and Data Science" fig-align="center"}

It has been a little while since my last installment ([Part II](https://towardsdatascience.com/similarity-measures-and-graph-adjacency-with-sets-a33d16e527e1 "Similarity Measures and Graph Adjacency with Sets")). End of year projects, deadlines, and (of course) the holidays have kept me busy. My thanks for your patience, and I'll try and make it worth the wait.

Last time, we talked about different similarity metrics for sets or combinatorial problems and their particular strengths and idiosyncrasies. We were trying to find the best way to project our bipartite graph of archaeological proveniences and artifacts into individual one-mode graphs. Our goal is to find communities in each of those projected graphs to find meaningful *assemblages* (communities of artifacts) and *contexts* (communities of provenience locations).

That does, however, require that we talk about how we determine "best" for this particular scenario. Since our goal is community detection, we want to choose the metric that connects vertices that have the strongest associations while *avoiding* spurious or non-informative edges. I mentioned last time that we are, in part, looking to see if our projected graphs exhibit characteristics of a *scale-free* network. We'll talk about that more later in this post, but it boils down to finding nodes that have a few strong connections rather than a lot of weak ones.

Since this is likely to be a fairly large and involved topic, I'm breaking Part III into two posts. This one will talk about translating our similarity measures into a network, and building the graphs. The next one will tackle the community detection methods, of which there are many.

For those just joining in, this is the Part III of a series in archaeological data science covering:

-   [Part I](https://medium.com/p/648a2f20d389 "Bipartite Graphs for Archaeological Assemblage Networks") -- Creating and exploring bipartite and one-mode graphs,

-   [Part II](https://towardsdatascience.com/similarity-measures-and-graph-adjacency-with-sets-a33d16e527e1 "Similarity Measures and Graph Adjacency with Sets") -- Similarity measures for sets and graph adjacency,

-   Part III -- Graph structure and community detection methods,

    -   a.) Scale-free and small world networks

    -   b.) Community detection methods

-   Part IV -- Geo-spatial networks

I'll continue using R for the [coding](https://github.com/archaeojsc/assemblage_network "Assemblage Network GitHub"), but all of this could be done with Python just as easily.

## Introduction

In [Part II](https://towardsdatascience.com/similarity-measures-and-graph-adjacency-with-sets-a33d16e527e1 "Similarity Measures and Graph Adjacency with Sets"), we went into some depth about what it means for sets to be *similar* and some ways to calculate a metric for that similarity. This time around, we're going to look more closely into how to decide what it means to be similar *enough*. In other words, we want to find an analytically valid way to decide on a *threshold* of similarity that best captures the real connections between entities while avoiding spurious associations. That will also help to guide us in selecting which of the similarity metrics to use (i.e., overlap, Jaccard, or Sørensen--Dice) as we go on to start building our bipartite graph projections.

### A quick recap...

We started (in [Part I](https://medium.com/p/648a2f20d389 "Bipartite Graphs for Archaeological Assemblage Networks")) by building a bipartite graph between artifact types and their locations, shown in @fig-bipartite-graph. To keep things simple, we're using an *un-*weighted bipartite graph to look at co-locations of artifact types.

```{r}
#| label: code-bpg
#| echo: false
#| message: false

require(tidyverse)
require(igraph)
require(ggraph)

# Import data from file ---------------------------------------------------

dat <- read_csv("Catalog_SiteA.csv",
                col_select = c(LEVEL_ID, CODE))

# Create un-weighted bipartite graph --------------------------------------

g_assemblages_bpg <-
  graph_from_data_frame(unique.data.frame(select(dat, LEVEL_ID, CODE)),
                        directed = FALSE)

V(g_assemblages_bpg)$type <-
  bipartite_mapping(g_assemblages_bpg)$type

```

```{r}
#| label: fig-bipartite-graph
#| fig-cap: "Bipartite network of Provenience and Artifact Type."
#| fig-alt: "Bipartite graph plot of proveniences and artifact types."
#| echo: false
#| message: false
#| warning: false

g_assemblages_bpg_layout <-
  g_assemblages_bpg %>% layout_as_bipartite()

g_assemblages_bpg %>%
  ggraph(layout = g_assemblages_bpg_layout) +
  geom_edge_link(edge_color = "gray", edge_alpha = 0.25) +
  geom_node_point(aes(color = type)) +
  scale_color_manual(
    values = c("green", "blue"),
    name = "Node Type",
    breaks = c(FALSE, TRUE),
    labels = c("Provenience", "Artifact")
  ) +
  ggtitle("Bipartite network of Provenience and Artifact Type")


```

Looking at the bipartite graph, it seems apparent that there are some natural clusters or groupings. We found, though, that projecting this graph into its one-mode counterparts resulted in very densely connected networks.

Next, in [Part II](https://towardsdatascience.com/similarity-measures-and-graph-adjacency-with-sets-a33d16e527e1 "Similarity Measures and Graph Adjacency with Sets"), we looked at different similarity metrics to find a better method of projection that the simple overlap provided in `igraph`. We looked at the similarities calculated with the Szymkiewicz-Simpson overlap coefficient, Jaccard similarity coefficient, and Sørensen--Dice coefficient for both provenience (@fig-sim-prov) and artifact types (@fig-sim-artifact).

```{r}
#| label: code-reestablish-environment
#| echo: false
#| message: false
#| warning: false

# Create incidence matrix from bipartite graph -----------------------------

g_assemblages_bpg_inc <- as_incidence_matrix(g_assemblages_bpg)

# Project one-mode graphs, Szymkiewicz-Simpson -----------------------------

overlap_coef_bin <- function(x) {
  # Calculate the pairwise sums of non-zero matrix elements to find the number
  # of intersecting elements between each input column
  bin_intersect_mat <- t(x) %*% x
  
  # Calculate the input column sums to find the individual size of each set
  col_sum <- apply(x, 2, function(xx)
    sum(xx != 0))
  
  # Find the smaller of each pair of sets by taking the matrix outer minimum
  min_set_size_mat <- outer(col_sum, col_sum, FUN = pmin)
  
  # Szymkiewicz-Simpson overlap coefficient is the pairwise intersection of two
  # sets divided by the size of the smaller set
  res <- bin_intersect_mat / min_set_size_mat
  
  # Set diagonal to identity
  diag(res) <- 1L
  
  # Assign input column names to rows and columns of return matrix
  dimnames(res) <- list(colnames(x), colnames(x))
  
  return(res)
}


## Project provenience -----------------------------------------------------

prov_adj_ssoc <- overlap_coef_bin(t(g_assemblages_bpg_inc))

prov_ssoc_vals <-
  prov_adj_ssoc[lower.tri(prov_adj_ssoc, diag = FALSE)]

## Project artifact types --------------------------------------------------

artifact_adj_ssoc <- overlap_coef_bin(g_assemblages_bpg_inc)

artifact_ssoc_vals <-
  artifact_adj_ssoc[lower.tri(artifact_adj_ssoc, diag = FALSE)]

# Project one-mode graphs, Sorenson-Dice -----------------------------------

soren_dice_sim_bin <- function(x) {
  # Calculate the pairwise sums of non-zero matrix elements to find the number
  # of intersecting elements between each input column
  bin_intersect_mat <- t(x) %*% x
  
  # Calculate the input column sums, to find the individual size of each set
  col_sum <- apply(x, 2, function(xx)
    sum(xx != 0))
  
  # Calculate the matrix outer sums for pairwise sum of set sizes
  set_size_sum_mat <- outer(col_sum, col_sum, FUN = "+")
  
  # Sorenson-Dice index is twice the size of the intersection divided by the
  # sum of the size for each set
  res <- (2 * bin_intersect_mat) / set_size_sum_mat
  
  # Set diagonal to identity
  diag(res) <- 1L
  
  # Assign input column names to rows and columns of return matrix
  dimnames(res) <- list(colnames(x), colnames(x))
  return(res)
  
}

## Project provenience -----------------------------------------------------

prov_adj_sd <- soren_dice_sim_bin(t(g_assemblages_bpg_inc))

prov_sd_vals <-
  prov_adj_sd[lower.tri(prov_adj_sd, diag = FALSE)]

## Project artifact types --------------------------------------------------

artifact_adj_sd <- soren_dice_sim_bin(g_assemblages_bpg_inc)

artifact_sd_vals <-
  artifact_adj_sd[lower.tri(artifact_adj_sd, diag = FALSE)]

# Project one-mode graphs, Jaccard ----------------------------------------

jaccard_sim_bin <- function(x) {
  # Calculate the pairwise sums of non-zero matrix elements to find the number
  # of intersecting elements between each input column
  bin_intersect_mat <- t(x) %*% x
  
  # Calculate the input column sums to find the individual size of each set
  x_col_sum <- apply(x, 2, function(xx)
    sum(xx != 0))
  
  # Calculate the matrix outer sums for pairwise sum of set sizes
  set_size_sum_mat <- outer(x_col_sum, x_col_sum, FUN = "+")
  
  # Jaccard index is intersection of set sizes over the size of the union of
  # sets
  res <- bin_intersect_mat / (set_size_sum_mat - bin_intersect_mat)
  
  # Set diagonal to identity
  diag(res) <- 1L
  
  # Assign input column names to rows and columns of return matrix
  dimnames(res) <- list(colnames(x), colnames(x))
  
  return(res)
}

## Project provenience -----------------------------------------------------

prov_adj_jacc <- jaccard_sim_bin(t(g_assemblages_bpg_inc))

prov_jacc_vals <-
  prov_adj_jacc[lower.tri(prov_adj_jacc, diag = FALSE)]

## Project artifact types --------------------------------------------------

artifact_adj_jacc <- jaccard_sim_bin(g_assemblages_bpg_inc)

artifact_jacc_vals <-
  artifact_adj_jacc[lower.tri(artifact_adj_jacc, diag = FALSE)]

# Comparing similarity measures -------------------------------------------

prov_sims <-
  data.frame(ssoc = prov_ssoc_vals,
             jacc = prov_jacc_vals,
             sd = prov_sd_vals)

artifact_sims <-
  data.frame(ssoc = artifact_ssoc_vals,
             jacc = artifact_jacc_vals,
             sd = artifact_sd_vals)

```

```{r}
#| label: fig-sim-prov
#| fig-cap: "Similarity measures for proveniences."
#| fig-alt: "Similarity measures for proveniences."
#| echo: false
#| message: false
#| warning: false

# Plot densities by similarity measure
prov_sims %>% stack() %>%
  ggplot(aes(x = values)) +
  geom_density(color = "green",
               alpha = 0.4) +
  facet_grid(ind ~ ., scales = "free") +
  ggtitle("Similarity Measures for Proveniences")

```

For proveniences, it looks like there are quite a lot that have *some* similarities in their artifact content.

```{r}
#| label: fig-sim-artifact
#| fig-cap: "Similarity measures for artifact types."
#| fig-alt: "Similarity measures for artifact types."
#| echo: false
#| message: false
#| warning: false

# Plot densities by similarity measure
artifact_sims %>% stack() %>%
  ggplot(aes(x = values)) +
  geom_density(color = "blue",
               alpha = 0.4) +
  facet_grid(ind ~ ., scales = "free") +
  ggtitle("Similarity Measures for Artifact Types")

```

In contrast to proveniences, the artifact types show far *less* similarity in where they co-occur.

Now what we want to do is look at how to go about selecting our similarity metric, how to find the appropriate thresholds of similarity, and what effects these have on the resulting graph projections. The goal is to find the right metric and threshold that will allow us to detect the underlying structure of the relationships within and between proveniences and artifact types.

## Graph structure and scale-free networks

What we're trying to do is find out if the inherent structure of our projected graphs, and therefore our bipartite graph as a whole, shows whether or not it is *random*. If the structure does *not* appear to be random, then we can reasonably infer that some other process is constraining or directing which artifacts appear where. In archaeological terms, that means that it's likely that there is some intentional organization to the use of space for certain activities or that certain artifact assemblages are associated with different activities. If so, then we can find some partitioning of the artifacts and/or proveniences that have meaningful interpretations.

Mind you, we're not actually trying to *induce* a structure. We are, however, expecting that there has been some manner of degradation in the "signal" of our data (see the introduction to [Part I](https://medium.com/p/648a2f20d389 "Bipartite Graphs for Archaeological Assemblage Networks") as to why). Our selection of similarity and threshold are meant to dampen the "noise" introduced to the site over time.

One way to determine if we have non-random structure in our graph is to find out if it exhibits [*scale-free network*](https://en.wikipedia.org/wiki/Scale-free_network "Scale-free network | WikipediA") properties. A scale-free network is one in which the distribution for the degree of nodes in the graph approximate a power-law distribution, meaning that most nodes in the graphs have few connections while a few "hub" nodes have many. In a random network, there would be a more *even*ly distributed probability of node degrees and hub nodes would be far less likely to occur.

The presence of a scale-free network suggests that there is some latent *non*-random process at play that is driving differential attachments between those nodes. It may be that there is a different process of growth in the network, which archaeologically would be some different spatial or temporal process of artifact deposition. Alternatively, there may be a clustering process or preferential attachment by which certain nodes are more closely associated than others as in the case of hub nodes. A certain suite of artifacts that are all related to the same activities (i.e., an *assemblage*) would cluster together, for example.

### Detecting scale-free networks

If a network is scale-free, then the distribution of node degree will approximate a power-law distribution of the form $P_{\text{deg}}(k) \propto k^{-\gamma}$ where $k$ is node degree and $\gamma$ is some exponent. We will know $k$, but we would obviously need to find the appropriate value for $\gamma$ from our empirical distribution. To illustrate, lets look at the probability density function for various values of $\gamma$ (@fig-power-law). We have 251 artifact types in our archaeological sample data, so lets look at the ideal power-law distributions if each node could have a maximum of 251 degrees.

```{r}
#| label: fig-power-law
#| fig-cap: "Power-law distribution for different values of gamma."
#| fig-alt: "Power-law distribution for different values of gamma."
#| echo: false
#| message: false
#| warning: false

powerlaw_pdf <- function(x, gamma = 1) {
  density <- x ^ (-gamma)
  return(density)
}

ggplot(data.frame(x = c(1, 251)), aes(x = x)) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 2), aes(color = "2")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 3), aes(color = "3")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 4), aes(color = "4")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 5), aes(color = "5")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 6), aes(color = "6")) +
  labs(color = "Gamma") +
  ylab("P(x)")

```

Pretty easy to see that if our artifact type graph were scale free, the vast majority of the nodes would have a *very* small degree. It's a little easier to see the effect of $\gamma$ if we take the $\log$ of $P_{\text{deg}}(k)$ (@fig-power-law-log).

```{r}
#| label: fig-power-law-log
#| fig-cap: "Log-scale power-law distribution for different values of gamma."
#| fig-alt: "Log-scale power-law distribution for different values of gamma."
#| echo: false
#| message: false
#| warning: false

ggplot(data.frame(x = c(1, 251)), aes(x = x)) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 2), aes(color = "2")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 3), aes(color = "3")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 4), aes(color = "4")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 5), aes(color = "5")) +
  stat_function(fun = powerlaw_pdf, args = list(gamma = 6), aes(color = "6")) +
  scale_y_continuous(trans="log10") +
  labs(color = "Gamma") +
  ylab("log P(x)")

```

Much easier to see that for every increment of the exponent $\gamma$, the probability of high-degree nodes decreases much more quickly. This will help in selecting our similarity metric, when we look at which one produces a graph network that follows this sort of power-law.
